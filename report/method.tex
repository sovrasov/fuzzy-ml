\section{Метод решения задачи}
Рассмотрим формальную постановку задачи классификации ирисов: требуется аппроксимировать зависимость
\(f :X \rightarrow Y\),
где \(X\) --- векторное пространство \(R^4\), а \(Y\) --- множество \(\{1,2,3\}\),
каждому элементу которого соответствует вид ирисов. Значения функции \(f\) известны
на конечном множестве элементов \(D=\{x(t):x(t)\in R^4, 1 \leqslant t \leqslant N\}\),
которое яляется обучающей выборкой.
\par
Модель TSK0 решает задачу регрессии, поэтому будем считать, что если на векторе \(x^*\)
модель выдала ответ \(y^*\), то \(\tilde y = \ceil{y^*-0.5}\) --- ответ в задаче классификации.
Таким образом, значения выходной переменной модели, лежащие в отрезке \([i-0.5;i+0.5]\)
соответствуют классу \(i\).
\par
На этапе предварительной обработки данных происходит нормализация элементов обучающей выборки.
С помощью линейного преобразования
\begin{displaymath}
x_i(t)^* = \frac{\displaystyle x_i(t) - \min_{t=\overline{1,N}} x_i(t)}{ \\*
\displaystyle \max_{t=\overline{1,N}} x_i(t)-\min_{t=\overline{1,N}} x_i(t)}
,i=\overline{1,4}
\end{displaymath}
значения координат входных векторов переводятся в единичный гиперкуб \([0;1]^4\).
\par
Модель TSK0 включает в себя правила нечёткого вывода вида
\begin{displaymath}
\logicIf(x_1 \is A_1^k) \land (x_2 \is A_2^k) \land (x_3 \is A_3^k) \land (x_4 \is A_4^k)\Rightarrow y(x_1,x_2,x_3,x_4)=b_k,
k=\overline{1,K}
\end{displaymath}
Функции принадлежностей термов в антецедентах правил вывода являются гауссовыми:
\begin{displaymath}
\mu_{i,k}(x_i)=\exp \left( \frac{(x_i-c_{i,k})^2}{2a_{i,k}^2} \right)
\end{displaymath}
\par
Чтобы произвести структурную идентификацию модели, надо определить количество правил \(K\) и
найти все параметры \(c_{i,k},a_{i,k},b_k\). Для этого производится кластеризация
входных данных с помощью конкурентного обучения нейронной сети Кохонена \cite{neuralNetworks}.
Алгоритм кластеризации описан в разделе \ref{subse:kohonen}.
Пусть в результате обучения сети были получены \(K\) центров кластеров в гиперкубе \([0;1]^4\).
Тогда каждому правилу нечёткой модели ставится в соответствие кластер, параметры \(c_{i,k}\)
для этого правила совпадают с координатами кластера. Параметры \(a_{i,k}\) выбираются
по правилу ближайшего соседа:
\begin{displaymath}
a_{i,k}=\frac{\Vert s_k-s_h\Vert}{1.5},h=\argmin_{j}\Vert s_k-s_j\Vert
\end{displaymath}
где \(s_j\) --- центр \(j\)-го кластера, \(\Vert * \Vert\) --- квадрат евклидовой нормы.
\par
Параметры \(b_k\) вычисляются как нормированная взвешенная сумма значений выходной переменной \(y\)
на обучающей выборке. В качестве весов здесь выступают значения функции истинности левой части
правила \(k\) на элементах выборки:
\begin{displaymath}
  b_k=\frac{\sum_{t=1}^{N}[\alpha_k(x(t))y(t)]}{\sum_{t=1}^{N}\alpha_k(x(t))},
  \alpha_k(x)=\prod_{j=1}^{4}\mu_{j,k}(x_j)
\end{displaymath}
\par
Выход модели TSK0 вычисляется по формуле:
\begin{displaymath}
  b_k=\frac{\sum_{k=1}^{K}[\alpha_k(x)b_k]}{\sum_{k=1}^{K}\alpha_k(x)}
\end{displaymath}
и является нормированной взвешенной суммой коэффициентов \(b_k\), где в
качестве весов выступают степени уверенности в антецеденте каждого правила модели.
\par
После структурной идентификации для более точного определения параметров
производится оптимизация по какому-либо критерию качества. В данном
случае ипользуется функция эмпирического риска \(Q(w)=\frac{1}{N}\sum_{t=1}^{N}L(y(t)-f(x(t),w))\),
где \(w\) --- вектор параметров модели, \(L(x,y)=(x-y)^2\). Оптимизация производится
методом роя частиц \cite{pso}. Размерность пространства параметров: \(K(2*4 + 1)\).
Сам метод описан в разделе \ref{subse:pso}.
\par
Для финальной оценки качества конкретного классификатора в данной работе применяется
метод одного выстрела. Выбирается число \(l:1\leqslant l\leqslant N\),
затем множество \(D\) разбивается на \(D_{train} = \{x(t):x(t)\in R^4, 1 \leqslant t \leqslant l-1\}\)
и \(D_{test} = \{x(t):x(t)\in R^4, l \leqslant t \leqslant N\}\). Выборка \(D_{train}\)
используеся для структурной идентификации модели и при параметрической оптимизации.
По выборке \(D_{test}\) вычисляется значение функционала \(Q(w)\).
\par
Чтобы оценить процесс построения модели в целом, а не качество конкретного классификатора,
используется метод перекрёстного контроля, состоящий в разбиении исходной выборки
\(D\) на \(m>1\) непересекающихся частей \(D_j\) и последовательном использовании
каждой из частей в качестве тестовой выборки, а объединения всех остальных в роли
обучающей. Таким образом, метод предполагает построение и оценку \(m\) моделей, а
затем усреднение этой оценки: \(Q(w)=\frac{1}{m} \sum_{i=1}^{m}Q_i(w)\).

\subsection{Конкурентное обучение сети Кохонена}
\label{subse:kohonen}
Сеть является двухслойной полносвязной. Первый слой входной, на него поступают компоненты
векторов пространства \(R^N\). Каждый нейрон первого слоя связан с \(H\) нейронами второго
слоя, представляющими собой центры кластеров в \(R^N\). Выход сети вычисляется по правилу
<<победитель получает всё>>: на нейроне второго слоя с номером
\(\displaystyle j=\argmax_{i=\overline{1,H}}(\Vert x - c_i \Vert)\) выход равен единице, а на остальных нулю.
Таким образом, сеть позволяет кластеризовать данные, поступающие на вход.
\par
Сеть Кохонена может обучаться без учителя. Для этого используется алгоритм конкурентного
обуения. Пусть сгенерированы начальные координаты центров кластеров \(c_i\), заданы
параметры \(\alpha_r < \alpha_w \in [0;1]\), обучающая выборка \(D=\{x(t):x(t)\in R^N, 1 \leqslant t \leqslant N\}\).
Для каждой частицы определён счётчик побед \(n_k\), задано максимальное количество итераций
\(maxIterations\) и точность \(\varepsilon\).
\newline
Пока не выполнен критерий остановки:
\begin{enumerate}
  \item \(epochNumber+=1\)
  \item \(\forall x(t)\in D\)
    \begin{enumerate}
      \item \(\displaystyle w=\argmin_{k=\overline{1,H}}d(x(t),c_k)\), \(\displaystyle d(x(t),c_k)=\frac{n_k}{\sum_{i=1}^{H}n_i}\Vert x(t)-c_k \Vert^2\).
      \item \(\displaystyle r=\argmin_{k\neq w}d(x(t),c_k)\)
      \item \(n_w +=1\)
      \item \(c_w=c_w+\alpha_w (x(t)-c_w)\)
      \item \(c_r=c_r-\alpha_r (x(t)-c_r)\)
    \end{enumerate}
  \item \(\displaystyle\alpha_w = \alpha_w - \alpha_w \frac{epochNumber}{maxIterations}\)
  \item \(\displaystyle\alpha_r = \alpha_r - \alpha_r \frac{epochNumber}{maxIterations}\)
  \item Проверить критерий остановки: \(\displaystyle\frac{1}{H}\sum_{k=1}^{H} \Vert c_k^{epochNumber} - c_k^{epochNumber-1} \Vert^2 < \varepsilon \)
\end{enumerate}
Из вида формул для обновления \(\alpha_w\) и \(\alpha_r\) следует, что остановка
алгоритма гарантированно произойдёт не позднее, чем через \(maxIterations\) итреаций.

\subsection{Алгоритм роя частиц}
\label{subse:pso}
Алгоритм роя частиц является прямым стохастическим алгоритмом оптимизации. Он производит
случайный направленный поиск в многомерном пространостве, моделируя поведение некоторой
социальной группы.
\par
Пусть есть \(S\) частиц с координатами \(x_i(0)\) и скоростями \(v_i(0),i=\overline{1,s}\).
В начальный момент \(t=0\) все частицы лежат в допустимой области \(G\), их скорости нулевые.
Далее пока не выполнен критерий остановки, координаты частиц и их скорости обновляются по следующим формулам:
\begin{displaymath}
  \begin{array}{l}
v_i(t+1)=w*v_i(t) + c_1*rand()*(p_i(t)-x_i(t)) + c_2*rand()*(p_g(t)-x_i(t)) \\*
x_i(t+1)= x_i(t) + v_i(t)
\end{array}
\end{displaymath}
Здесь \(w,\) \(c_1,\) \(c_2 \in [0;1]\) --- параметры алгоритма, функция \(rand()\) возвращает число из
\([0;1]\), \(p_i(t)\) --- лучшее положение \(i\)-ой частицы на момент \(t\),  \(p_g(t)\) ---
лучшее положение среди всех частиц роя на момент \(t\).
Если частица \(x_i(t)\) выйдет за пределы \(G\) при прибавлении к ней вектора скорости \(v_i(t)\),
необходимо последовательно уменьшать \(v_i(t)\), пока \(x_i(t+1)\) не будет лежать в \(G\).
\par
В качестве критерия остановки можно использовать достижение максимального числа итераций,
приемлемого значения целевой функции или малость изменения значения целевой функции для лучшей
частицы роя.
За оценку оптимума после окончания работы принимается точка \(p_g(t)\).
