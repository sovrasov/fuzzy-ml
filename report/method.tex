\section{Метод решения задачи}
Рассмотрим формальныю постановку задачи классификации ирисов: требуется аппроксимировать зависимость
\(f :X \rightarrow Y\),
где \(X\) --- векторное пространство \(R^4\), а \(Y\) --- множество \(\{1,2,3\}\),
каждому элементу которого соответствует вид ирисов. Значения функции \(f\) известны
на конечном множестве элементов \(D=\{x(t):x(t)\in R^4, 1 \leqslant t \leqslant N\}\),
которое яляется обучающей выборкой.
\par
Модель TSK0 решает задачу регрессии, поэтому будем считать, что если на векторе \(x^*\)
модель выдала ответ \(y^*\), то \(\tilde y = \ceil{y^*-0.5}\) --- ответ в задаче классификации.
Таким образом, значения выходной переменной модели, лежащие в отрезке \([i-0.5;i+0.5]\)
соответствуют классу \(i\).
\par
На этапе предварительной обработки данных происходит нормализация элементов обучающей выборки.
С помощью линейного преобразования
\begin{displaymath}
x(t)_i^* = \frac{\displaystyle x(t)_i - \min_{t=\overline{1,N}} x(t_i)}{ \\*
\displaystyle \max_{t=\overline{1,N}} x(t_i)-\min_{t=\overline{1,N}} x(t_i)}
,i=\overline{1,4}
\end{displaymath}
значения координат входных векторов переводятся в единичный гиперкуб \([0;1]^4\).
\par
Модель TSK0 включает в себя правила нечёткого вывода вида
\begin{displaymath}
\logicIf(x_1 \is A_1^k) \land (x_2 \is A_2^k) \land (x_3 \is A_3^k) \land (x_4 \is A_4^k)\Rightarrow y(x_1,x_2,x_3,x_4)=b_k,
k=\overline{1,K}
\end{displaymath}
Функции принадлежностей термов в антецедентах правил вывода являются гауссовыми:
\begin{displaymath}
\mu_{i,k}(x_i)=\exp \left( \frac{(x_i-c_{i,k})^2}{2a_{i,k}^2} \right)
\end{displaymath}
\par
Чтобы произвести структурную идентификацию модели, надо определить количество правил \(K\) и
найти все параметры \(c_{i,k},a_{i,k},b_k\). Для этого производится кластеризация
входных данных с помощью конкурентного обучения нейронной сети Кохонена \cite{neuralNetworks}.
Пусть в результате обучения были получены \(K\) центров кластеров в пространстве \(R^4\).
Тогда каждому правилу нечёткой модели ставится в соответствие кластер, параметры \(c_{i,k}\)
для этого правила совпадают с координатами кластера. Параметры \(a_{i,k}\) выбираются
по правилу ближайшего соседа:
\begin{displaymath}
a_{i,k}=\frac{\Vert s_k-s_h\Vert}{1.5},h=\argmin_{j}\Vert s_k-s_j\Vert
\end{displaymath}
где \(s_j\) --- центр \(j\)-го кластера, \(\Vert * \Vert\) --- квадрат евклидовой нормы.
\par
Параметры \(b_k\) вычисляются как нормированная взвешенная сумма значений выходной переменной \(y\)
на обучающей выборке. В качестве весов здесь выступают значения функции истинности левой части
правила \(k\) на элементах выборки:
\begin{displaymath}
  b_k=\frac{\sum_{t=1}^{N}[\alpha_k(x(t))y(t)]}{\sum_{t=1}^{N}\alpha_k(x(t))},
  \alpha_k(x)=\prod_{j=1}^{4}\mu_{j,k}(x_j)
\end{displaymath}
\par
Выход модели TSK0 вычисляется по формуле:
\begin{displaymath}
  b_k=\frac{\sum_{k=1}^{K}[\alpha_k(x)b_k]}{\sum_{k=1}^{K}\alpha_k(x)}
\end{displaymath}
и является нормированной взвешенной суммой коэффициентов \(b_k\), где в
качестве весов выступают степени уверенности в антецеденте каждого правила модели.
\par
После структурной идентификации для более точного определения параметров
производится оптимизация по какому-либо критерию качества. В данном
случае ипользуется функция эмпирического риска \(Q(w)=\frac{1}{N}\sum_{t=1}^{N}L(y(t)-f(x(t),w))\),
где \(w\) --- вектор параметров модели, \(L(x,y)=(x-y)^2\). Оптимизация производится
методом роя частиц \cite{pso}.
\par
Для финальной оценки качества конкретного классификатора в данной работе применяется
метод одного выстрела. Выбирается число \(l:1\leqslant l\leqslant N\),
затем множество \(D\) разбивается на \(D_{train} = \{x(t):x(t)\in R^4, 1 \leqslant t \leqslant l-1\}\)
и \(D_{test} = \{x(t):x(t)\in R^4, l \leqslant t \leqslant N\}\). Выборка \(D_{train}\)
используеся для структурной идентификации модели и при параметрической оптимизации.
По выборке \(D_{test}\) вычисляется значение функционала \(Q(w)\).
\par
Чтобы оценить процесс построения модели в целом, а не качество конкретного классификатора,
в используется метод перекрёстного контроля, состоящий в разбиении исходной выборки
\(D\) на \(m>1\) непересекающихся частей \(D_j\) и последовательном использовании
каждой из частей в качестве тестовой выборки, а объединения всех остальных в роли
обучающей. Таким образом, метод предполагает построение и оценку \(m\) моделей, а
затем усреднение этой оценки: \(Q(w)=\frac{1}{m} \sum_{i=1}^{m}Q_i(w)\).

\subsection{Конкурентное обучение сети Кохонена}
\label{subse:kohonen}
\subsection{Алгоритм роя частиц}
\label{subse:pso}
